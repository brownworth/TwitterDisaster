{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.casual import reduce_lengthening\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from math import ceil\n",
    "from sklearn.metrics import (f1_score, classification_report, \n",
    "                            confusion_matrix, accuracy_score, \n",
    "                            precision_score, recall_score, \n",
    "                            roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.spatial.distance import cosine\n",
    "import string\n",
    "from math import log10\n",
    "from scipy.stats import norm\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Tweet Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_en = tweet_full[tweet_full['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_full_en.loc[tweet_date:tweet_date + date_delta,\"tweet_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "##     takes input string and converts or removes characters depending on settings.\n",
    "##     returns a string\n",
    "##     convert case:\n",
    "    tweet = tweet.lower()\n",
    "##    remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "##     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "##     remove non-hashtag punctuation:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "##     convert non-hashtag punctuation to whitespace:\n",
    "    tweet = tweet.translate(punc_test.maketrans(string.punctuation.replace(\"#\",\"\"),\" \"*len(string.punctuation.replace(\"#\",\"\"))))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('\\B#\\B','',tweet)\n",
    "##     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "##     drop numbers and words of < 4 characters.\n",
    "#     tweet = re.sub(r'\\b(?<!#)\\w{1,3}\\b','',tweet)\n",
    "    tweet = re.sub(r'\\b(?<!#)\\d+\\b','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_reduce_len = reduce_lengthening(cleaned_tweet)\n",
    "#     tweet_as_tokens = word_tokenize(tweet_reduce_len)\n",
    "    tweet_as_tokens = tweet_reduce_len.split()\n",
    "    tweet_no_stops = [stemmer.stem(word) for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous Tests:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_test = \"This is some (&) text$that has &*char$ in it\"\n",
    "punc_test.translate(punc_test.maketrans(string.punctuation.replace(\"#\",\"\"),\" \"*len(string.punctuation.replace(\"#\",\"\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_test.translate(punc_test.maketrans('','',string.punctuation.replace(\"#\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_hash_test = '# #34 4#3 A#36 3 A# #hashtag'\n",
    "print(re.sub(r'\\b#\\B','!',re_hash_test))\n",
    "print(re.sub(r'\\b#\\b','!',re_hash_test))\n",
    "print(re.sub(r'\\B#\\B','!',re_hash_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"this is a tweet # #### ####1 #hashtag #123 #12345 apm yooooo\"\n",
    "tokens_no_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word/Token Statistics\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = tweet_text.str.split(\"\\s+\").apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_reduce_len = TweetTokenizer(reduce_len=True)\n",
    "word_counts_reduce = tweet_text.apply(reduce_lengthening).str.split().apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts_reduce.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(word_counts.index.values,word_counts_reduce.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_reduce.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts_full_tokenizer = tweet_text.apply(tweet_tokenizer.tokenize).apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts_full_tokenizer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts_full_tokenizer.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tweet_text.apply(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_test = tweet_text.loc['2017-09-10 00:03:40'][1]\n",
    "# print(tweet_test)\n",
    "# tweet_test = tweet_test.lower()\n",
    "# print(word_tokenize(tweet_test))\n",
    "# print(tweet_test.split())\n",
    "# i=0\n",
    "# print(f\"S: {tweet_test}\");i+=1\n",
    "# #     1: remove URLs:\n",
    "# tweet_test = re.sub('https?://\\S+','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     2: remove @mentions, including those with a leading '-' or '.' : \n",
    "# tweet_test = re.sub('[-\\.]?@\\w+','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     3: remove punctuation, but not hashtags:\n",
    "# tweet_test = tweet_test.translate(tweet_test.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     4: remove non-hashtag '#'.\n",
    "# # tweet_test = re.sub('#\\B','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     5: remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "# # tweet_test = re.sub(r'\\b(amp|gt|lt)\\b','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     6: drop words of < 4 characters, but not hashtags\n",
    "# tweet_test = re.sub(r'\\b(?<!#)\\w{1,3}\\b','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "# #     7: drop numbers, but not hashtags\n",
    "# tweet_test = re.sub(r'\\b(?<!#)\\d+\\b','',tweet_test)\n",
    "# print(f\"{i}: {tweet_test}\");i+=1\n",
    "\n",
    "# print(word_tokenize(tweet_test))\n",
    "# print(tokens_no_stopwords(tweet_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_test = '2017-09-10 00:03:40'\n",
    "print(tweet_text.loc[date_test].apply(reduce_lengthening))\n",
    "print(tweet_text.loc[date_test].apply(tokens_no_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets[tokenized_tweets.apply(lambda x: '@' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_full_token_clean = tweet_text.apply(tokens_no_stopwords).apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_full_token_clean.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_full_token_clean.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len = tweet_text.groupby(tweet_text.str.split().apply(len)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len_post = tweet_text.apply(tokens_no_stopwords).apply(len).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len_post[tweet_len_post.index <= 10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len_post.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_len_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_tweet_hist = plt.figure(figsize=(16,8))\n",
    "plt.style.use('bmh')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.bar(tweet_len.index,tweet_len.values,alpha=.7,label=\"Tweets by length\")\n",
    "plt.bar(tweet_len_post.index,tweet_len_post.values,alpha=.7,label=\"Tweets by length (after cleaning)\")\n",
    "plt.xlabel(\"Quantity of Tokens Per Tweet\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.title(\"Histogram of Tweets by Length (Token Quantity)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# fig_tweet_hist.savefig(f'./figures/TweetsByLength.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'irma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Size: 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_type = \"Min. Word Freq.\"\n",
    "# comparison_short = \"MinWordFreq\"\n",
    "comparison_type = \"Window Size\"\n",
    "comparison_short = \"WindowSize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminating the off-by-one error in the loop later. This will have a zero row, unfilled by vectors.\n",
    "vectors_series = pd.Series(np.zeros((11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'punc_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-74f7b4bba406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_full_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_date\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate_delta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tweet_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_no_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# ---------- Skip-Gram ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# initialize the Word2Vec vectors with window size i, min count 1, and Skip-Gram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6fb965848ed9>\u001b[0m in \u001b[0;36mtokens_no_stopwords\u001b[0;34m(tweet_as_string)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#     takes a string and returns a list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcleaned_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_as_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtweet_reduce_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_lengthening\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     tweet_as_tokens = word_tokenize(tweet_reduce_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6fb965848ed9>\u001b[0m in \u001b[0;36mclean_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m##     convert non-hashtag punctuation to whitespace:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#     remove non-hashtag '#'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\B#\\B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'punc_test' is not defined"
     ]
    }
   ],
   "source": [
    "tweet_text = tweet_full_en.loc[tweet_date:tweet_date + date_delta,\"tweet_text\"]\n",
    "tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "for i in range(1,11):\n",
    "    # ---------- Skip-Gram ----------\n",
    "    # initialize the Word2Vec vectors with window size i, min count 1, and Skip-Gram\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=1, window=i, workers=1, size=100, seed=1)#, sg=1, hs=1)\n",
    "    \n",
    "    # train the model over 10 epochs\n",
    "    vector_model.train(tweets_tokens, total_examples=len(tweet_text), epochs=10)\n",
    "    \n",
    "    # assign the vector data to its position in a series\n",
    "    vectors_series.iloc[i] = vector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_tweets = pd.read_csv(r'./irma_related_tweets.csv')\n",
    "\n",
    "tweets_on_date = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "\n",
    "tweet_encoded = pd.concat([coded_tweets.reset_index(),tweets_on_date.iloc[:-1].reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_type = \"Mean Cosine Similarity\"\n",
    "# func_short = \"MeanCosSim\"\n",
    "# def ScoreTweetFromVectors(tweet,vector_set):\n",
    "#     tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "#     score = 0\n",
    "#     for i in tweet_as_terms:\n",
    "#         if i in vector_set.wv.vocab:\n",
    "#             score += vector_set.wv.similarity(i,search_term)\n",
    "#     if len(tweet_as_terms) > 0:\n",
    "#         score /= len(tweet_as_terms)\n",
    "#     else:\n",
    "#         score = 0\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import sqrt\n",
    "# func_type = \"Sum Score over Sqrt Cos. Sim.\"\n",
    "# func_short = \"SumOverSqrtLen\"\n",
    "# def ScoreTweetFromVectors(tweet,vector_set):\n",
    "#     tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "#     score = 0\n",
    "#     for i in tweet_as_terms:\n",
    "#         if i in vector_set.wv.vocab:\n",
    "#             score += vector_set.wv.similarity(i,search_term)\n",
    "#     if len(tweet_as_terms) > 0:\n",
    "#         score /= sqrt(len(tweet_as_terms))\n",
    "#     else:\n",
    "#         score = 0\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_type = \"Dot Product of Tweet Vector and Search Term\"\n",
    "func_short = \"DotProduct\"\n",
    "def ScoreTweetFromVectors(tweet,vector_set):\n",
    "    tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "#     initialize vector with dimensionality of the vector set.\n",
    "    vector_dim = len(vector_set.wv.vectors[0])\n",
    "    score_matrix = np.zeros(vector_dim,) \n",
    "#     iterate over each word after processing. If the word is in the vocabulary,\n",
    "#     add its vector's value to the score matrix.\n",
    "#     this essentially treats a word not in the vocabulary as a zero-vector.\n",
    "    for i in tweet_as_terms:\n",
    "        if i in vector_set.wv.vocab:\n",
    "            score_matrix = np.add(score_matrix,vector_set.wv.get_vector(i))\n",
    "#     if the number of words remaining in the tweet after processing is equal to zero, return zero.\n",
    "#     otherwise, take the dot product of the score vector, and the vector of the search term.\n",
    "    if len(tweet_as_terms) > 0:\n",
    "        score = np.dot(score_matrix,vector_set.wv.get_vector(search_term))\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_type = \"Cosine Similarity of Tweet Vector Sum and Search Term\"\n",
    "# func_short = \"CosSimTweetVecSum\"\n",
    "# def ScoreTweetFromVectors(tweet,vector_set):\n",
    "#     tweet_as_terms = tokens_no_stopwords(tweet)\n",
    "# #     initialize vector with dimensionality of the vector set.\n",
    "#     vector_dim = len(vector_set.wv.vectors[0])\n",
    "#     score_matrix = np.zeros(vector_dim,) \n",
    "# #     iterate over each word after processing. If the word is in the vocabulary,\n",
    "# #     add its vector's value to the score matrix.\n",
    "# #     this essentially treats a word not in the vocabulary as a zero-vector.\n",
    "#     for i in tweet_as_terms:\n",
    "#         if i in vector_set.wv.vocab:\n",
    "#             score_matrix = np.add(score_matrix,vector_set.wv.get_vector(i))\n",
    "# #     if the number of words remaining in the tweet after processing is equal to zero, return zero.\n",
    "# #     otherwise, take the pairwise cosine of the score vector and the vector of the search term.\n",
    "#     if len(tweet_as_terms) > 0:\n",
    "#         score = 1 - cosine(score_matrix,vector_set.wv.get_vector(search_term))\n",
    "#     else:\n",
    "#         score = 0\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_encoded.iloc[0:5].tweet_text.apply(ScoreTweetFromVectors,args=(vectors_series.iloc[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "#     create strings for column heads\n",
    "    sw = f'score_window_{i}'\n",
    "    mmsw = f'MM_{sw}'\n",
    "    \n",
    "#     Scoring tweets in this column:\n",
    "    tweet_encoded[sw] = tweet_encoded.tweet_text.apply(ScoreTweetFromVectors,args=(vectors_series.iloc[i],))\n",
    "                                                       \n",
    "#     column of scores for this iteration\n",
    "    tweet_scores = tweet_encoded[sw]\n",
    "\n",
    "#     calculating Min Max Scaling for this column \n",
    "    tweet_encoded[mmsw] = ((tweet_scores - tweet_scores.min())* 100) / (tweet_scores.max() - tweet_scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_0 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "\n",
    "for j in range(1,11):\n",
    "    window_label = f\"{comparison_type}: {j}\"\n",
    "    plt.plot(range(101),[100*len(tweet_encoded[tweet_encoded[f'MM_score_window_{j}'] > i])/len(tweet_encoded) for i in range(101)],label=window_label)\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"Percent of Tweets at or above Threshold\")\n",
    "plt.title(f\"Percent of Tweets at or above Threshold by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "\n",
    "# fig_0.savefig(f'./figures/{comparison_short}{func_short}PercentTweetsBelow.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_window_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "pre_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "rec_score_df = pd.DataFrame(np.zeros((101,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    for j in range(101):\n",
    "        f1_window_score_df.iloc[j,i] = f1_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        pre_score_df.iloc[j,i] = precision_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        rec_score_df.iloc[j,i] = recall_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([f1_window_score_df.idxmax(),f1_window_score_df.max()],axis=1,names=['TweetScore','F1Score']))\n",
    "print(f1_window_score_df.idxmax().max())\n",
    "print(f1_window_score_df.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_1 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "for i in range(1,11):\n",
    "    plt.plot(f1_window_score_df.iloc[:,i],label=f\"{comparison_type}: {i}\")\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"F1 Score of Tweets Above Threshold\")\n",
    "plt.title(f\"F1 Score of Tweets by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "# fig_1.savefig(f'./figures/{comparison_short}{func_short}F1Score.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Dimensionality (window size 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_type = \"Hidden Layer Dimensionality\"\n",
    "comparison_short = \"HLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminating the off-by-one error in the loop later. This will have a zero row, unfilled by vectors.\n",
    "vectors_series = pd.Series(np.zeros((11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_full_en.loc[tweet_date:tweet_date + date_delta,\"tweet_text\"]\n",
    "tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "for i in range(1,11):\n",
    "    hidden_layer_dim = 25*i + 25\n",
    "    # ---------- Skip-Gram ----------\n",
    "    # initialize the Word2Vec vectors with window size 7, min count 1, and Skip-Gram (Hidden Layer Dim. Test)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=1, window=7, workers=1, size=hidden_layer_dim, seed=1, sg=1)\n",
    "    \n",
    "    # train the model over 10 epochs\n",
    "    vector_model.train(tweets_tokens, total_examples=len(tweet_text), epochs=10)\n",
    "    \n",
    "    # assign the vector data to its position in a series\n",
    "    vectors_series.iloc[i] = vector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_tweets = pd.read_csv(r'./irma_related_tweets.csv')\n",
    "\n",
    "tweets_on_date = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "\n",
    "tweet_encoded = pd.concat([coded_tweets.reset_index(),tweets_on_date.iloc[:-1].reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "#     create strings for column heads\n",
    "    sw = f'score_window_{i}'\n",
    "    mmsw = f'MM_{sw}'\n",
    "    \n",
    "#     Scoring tweets in this column:\n",
    "    tweet_encoded[sw] = tweet_encoded.tweet_text.apply(ScoreTweetFromVectors,args=(vectors_series.iloc[i],))\n",
    "                                                       \n",
    "#     column of scores for this iteration\n",
    "    tweet_scores = tweet_encoded[sw]\n",
    "\n",
    "#     calculating Min Max Scaling for this column \n",
    "    tweet_encoded[mmsw] = ((tweet_scores - tweet_scores.min())* 100) / (tweet_scores.max() - tweet_scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_hld_1 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "\n",
    "for j in range(1,11):\n",
    "    window_label = f\"{comparison_type}: {25*j + 25}\"\n",
    "    plt.plot(range(101),[len(tweet_encoded[tweet_encoded[f'MM_score_window_{j}'] > i])/len(tweet_encoded) for i in range(101)],label=window_label)\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"Percent of Tweets at or Above Threshold\")\n",
    "plt.title(f\"Percent of Tweets at or Above Threshold by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "\n",
    "# fig_hld_1.savefig(f'./figures/{comparison_short}{func_short}PercentTweetsBelow.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_window_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "pre_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "rec_score_df = pd.DataFrame(np.zeros((101,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    for j in range(101):\n",
    "        f1_window_score_df.iloc[j,i] = f1_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        pre_score_df.iloc[j,i] = precision_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        rec_score_df.iloc[j,i] = recall_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([f1_window_score_df.idxmax(),f1_window_score_df.max()],axis=1,names=['TweetScore','F1Score']))\n",
    "print(f1_window_score_df.idxmax().max())\n",
    "print(f1_window_score_df.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_hld_2 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "for i in range(1,11):\n",
    "    window_label = f\"{comparison_type}: {25*i + 25}\"\n",
    "    plt.plot(f1_window_score_df.iloc[:,i],label=window_label)\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"F1 Score of Tweets Above Threshold\")\n",
    "plt.title(f\"F1 Score of Tweets by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "# fig_hld_2.savefig(f'./figures/{comparison_short}{func_short}F1Score.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling (window size 7, dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_type = \"Negative Sampling Test\"\n",
    "comparison_short = \"NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminating the off-by-one error in the loop later. This will have a zero row, unfilled by vectors.\n",
    "vectors_series = pd.Series(np.zeros((11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_full_en.loc[tweet_date:tweet_date + date_delta,\"tweet_text\"]\n",
    "tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "for i in range(1,11):\n",
    "    ns_size = 10*i\n",
    "    # ---------- Skip-Gram ----------\n",
    "    # initialize the Word2Vec vectors with window size 7, min count 1, and Skip-Gram (Negative Sampling Test)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=1, window=7, workers=1, size=100, seed=1, sg=1, negative=ns_size)\n",
    "    \n",
    "    # train the model over 10 epochs\n",
    "    vector_model.train(tweets_tokens, total_examples=len(tweet_text), epochs=10)\n",
    "    \n",
    "    # assign the vector data to its position in a series\n",
    "    vectors_series.iloc[i] = vector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_tweets = pd.read_csv(r'./irma_related_tweets.csv')\n",
    "\n",
    "tweets_on_date = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "\n",
    "tweet_encoded = pd.concat([coded_tweets.reset_index(),tweets_on_date.iloc[:-1].reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "#     create strings for column heads\n",
    "    sw = f'score_window_{i}'\n",
    "    mmsw = f'MM_{sw}'\n",
    "    \n",
    "#     Scoring tweets in this column:\n",
    "    tweet_encoded[sw] = tweet_encoded.tweet_text.apply(ScoreTweetFromVectors,args=(vectors_series.iloc[i],))\n",
    "                                                       \n",
    "#     column of scores for this iteration\n",
    "    tweet_scores = tweet_encoded[sw]\n",
    "\n",
    "#     calculating Min Max Scaling for this column \n",
    "    tweet_encoded[mmsw] = ((tweet_scores - tweet_scores.min())* 100) / (tweet_scores.max() - tweet_scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_ns_1 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "\n",
    "for j in range(1,11):\n",
    "    window_label = f\"{comparison_type}: {10*j}\"\n",
    "    plt.plot(range(101),[100 * len(tweet_encoded[tweet_encoded[f'MM_score_window_{j}'] > i])/len(tweet_encoded) for i in range(101)],label=window_label)\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"Percent of Tweets Above Threshold\")\n",
    "plt.title(f\"Percent of Tweets Above Threshold by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "\n",
    "# fig_ns_1.savefig(f'./figures/{comparison_short}{func_short}PercentTweetsBelow.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_window_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "pre_score_df = pd.DataFrame(np.zeros((101,11)))\n",
    "rec_score_df = pd.DataFrame(np.zeros((101,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    for j in range(101):\n",
    "        f1_window_score_df.iloc[j,i] = f1_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        pre_score_df.iloc[j,i] = precision_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)\n",
    "        rec_score_df.iloc[j,i] = recall_score(tweet_encoded.irma_rel,tweet_encoded[f'MM_score_window_{i}'] > j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.concat([f1_window_score_df.idxmax().rename('MM_Score'),f1_window_score_df.max().rename('F1_Score_Max')],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_window_score_df.idxmax().max())\n",
    "print(f1_window_score_df.max().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_ns_2 = plt.figure(figsize=(16,8),facecolor='w')\n",
    "for i in range(1,11):\n",
    "    if i == f1_window_score_df.max().idxmax():\n",
    "        plt.plot(f1_window_score_df.iloc[:,i],label=f\"{comparison_type}: {10*i} (F1 Max)\",linestyle=\":\")\n",
    "    else:\n",
    "        plt.plot(f1_window_score_df.iloc[:,i],label=f\"{comparison_type}: {10*i}\")\n",
    "plt.xlabel(\"Min-Max Scaled Tweet Score Threshold\")\n",
    "plt.ylabel(\"F1 Score of Tweets Above Threshold\")\n",
    "plt.title(f\"F1 Score of Tweets by Word2Vec {comparison_type} ({func_type})\")\n",
    "plt.legend();\n",
    "# fig_ns_2.savefig(f'./figures/{comparison_short}{func_short}F1Score.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(tweet_encoded['irma_rel'],tweet_encoded['MM_score_window_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_roc_1 = plt.figure(figsize=(12,12),facecolor='w')\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_roc_1 = plt.figure(figsize=(12,12),facecolor='w')\n",
    "for i in range(1,11):\n",
    "    fpr, tpr, thresholds = roc_curve(tweet_encoded['irma_rel'],tweet_encoded[f'MM_score_window_{i}'])\n",
    "    plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_series.iloc[1].wv.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_series.iloc[1].wv.most_similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
