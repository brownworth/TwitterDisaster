{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Lists of Words Related to a Specified Search Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This algorithm provides the following:\n",
    "- Imports a dataframe of tweets\n",
    "- Processes tweets and tokenizes words\n",
    "- Uses Word Embeddings to convert words into vectors\n",
    "- Determines related words by using cosine similarity within the vector space\n",
    "- Generates a list, or lists, of words within the time delta(s) selected, ordered by min-maxed cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries\n",
    "Libraries and modules below are used to import and process the tokens into vectors, removing extraneous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Import\n",
    "Importing and parsing tweets into dataframe, converting the datestamp strings to datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating Tweets by language (english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_en = tweet_full[tweet_full['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Tokenization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "#     takes input string and converts or removes characters depending on settings.\n",
    "#     returns a string\n",
    "#     convert case:\n",
    "    tweet = tweet.lower()\n",
    "#     remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "#     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "#     remove punctuation, but not hashtags:\n",
    "    tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('#\\B','',tweet)\n",
    "#     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "#     drop numbers and words of < 4 characters.\n",
    "    tweet = re.sub(r'\\b\\w{1,3}\\b','',tweet)\n",
    "    tweet = re.sub(r'\\b\\d+\\b','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_as_tokens = tweet_tokenizer.tokenize(cleaned_tweet)\n",
    "    tweet_no_stops = [word for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Term\n",
    "This is the term that will serve as the comparison for all later lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"irma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Words Dataframe and Time Deltas\n",
    "This instantiates the dataframe for the related words and specifies the start, end, and time delta for the periods of related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Word list quantity\n",
    "This number specifies the number of words that will be returned in each list associated with the time periods specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_words = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "### Converting Words to Vectors using the 'Word2Vec' library\n",
    "\n",
    "- Iterate through each time period designated above\n",
    "- Apply the tokenization and cleaning functions\n",
    "- Convert the tokens to vectors using the following:\n",
    "    - Minimum count = 1\n",
    "    - Skip-Gram\n",
    "    - Window of 3\n",
    "    - 100D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for tweet_day in pd.date_range(start = tweet_date, end = end_date, freq = date_delta):\n",
    "    tweet_text = tweet_full_en.loc[tweet_day:tweet_day + date_delta,\"tweet_text\"]\n",
    "\n",
    "    tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=1, sg=1, window=7, workers=1, size=100, seed=1)\n",
    "    word_matrix = vector_model.wv[vector_model.wv.vocab]\n",
    "\n",
    "    terms_from_range = pd.DataFrame.from_records(vector_model.wv.most_similar(search_term,topn=top_num_words),\n",
    "                                                 columns=[tweet_day,\"score\"])\n",
    "\n",
    "    related_words = pd.concat([related_words,terms_from_range],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaling on Cosine Similarity Values\n",
    "Convert the cosine similarity to min-maxed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_values = related_words.iloc[:,1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "related_words.iloc[:,1::2] = (cos_sim_values - cos_sim_values.min())* 100 / (cos_sim_values.max() - cos_sim_values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waiting</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pass</td>\n",
       "      <td>99.054718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>away</td>\n",
       "      <td>83.869393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>live</td>\n",
       "      <td>80.350208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunday</td>\n",
       "      <td>77.679818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  2017-09-10 00:00:00       score\n",
       "0             waiting  100.000000\n",
       "1                pass   99.054718\n",
       "2                away   83.869393\n",
       "3                live   80.350208\n",
       "4              sunday   77.679818"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Relationships with Human Coded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the list of related words to identify an Irma Related tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a list of human coded tweets for 'irma related'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_tweets = pd.read_csv(r'./irma_related_tweets.csv')\n",
    "\n",
    "tweets_on_date = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "\n",
    "tweet_encoded = pd.concat([coded_tweets.reset_index(),tweets_on_date.iloc[:-1].reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts of Irma related tweets: 0 = not related, 1 = 'irma' related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "irma_rel\n",
       "0    10792\n",
       "1     8296\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_encoded.loc[:,['tweet_id','timestamp','irma_rel','tweet_text']].groupby('irma_rel').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waiting</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pass</td>\n",
       "      <td>99.054718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>away</td>\n",
       "      <td>83.869393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>live</td>\n",
       "      <td>80.350208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunday</td>\n",
       "      <td>77.679818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>part</td>\n",
       "      <td>75.084738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coverage</td>\n",
       "      <td>73.136757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ready</td>\n",
       "      <td>72.039836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>calm</td>\n",
       "      <td>66.456931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hours</td>\n",
       "      <td>64.606797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>close</td>\n",
       "      <td>64.416880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#irmahurricane</td>\n",
       "      <td>62.051127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>coming</td>\n",
       "      <td>59.251181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>update</td>\n",
       "      <td>58.843941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>side</td>\n",
       "      <td>58.246104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hunkered</td>\n",
       "      <td>58.131157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>morning</td>\n",
       "      <td>57.481566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>riding</td>\n",
       "      <td>57.417468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gets</td>\n",
       "      <td>57.237969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sound</td>\n",
       "      <td>56.983954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>almost</td>\n",
       "      <td>56.643190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>first</td>\n",
       "      <td>55.468241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>#hurrcaneirma</td>\n",
       "      <td>54.704836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>category</td>\n",
       "      <td>54.704270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>worst</td>\n",
       "      <td>53.554348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>#irma2017</td>\n",
       "      <td>52.584605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#hurricanirma</td>\n",
       "      <td>51.404446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>closer</td>\n",
       "      <td>50.684188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>little</td>\n",
       "      <td>50.556671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-10 00:00:00       score\n",
       "0              waiting  100.000000\n",
       "1                 pass   99.054718\n",
       "2                 away   83.869393\n",
       "3                 live   80.350208\n",
       "4               sunday   77.679818\n",
       "5                 part   75.084738\n",
       "6             coverage   73.136757\n",
       "7                ready   72.039836\n",
       "8                 calm   66.456931\n",
       "9                hours   64.606797\n",
       "10               close   64.416880\n",
       "11      #irmahurricane   62.051127\n",
       "12              coming   59.251181\n",
       "13              update   58.843941\n",
       "14                side   58.246104\n",
       "15            hunkered   58.131157\n",
       "16             morning   57.481566\n",
       "17              riding   57.417468\n",
       "18                gets   57.237969\n",
       "19               sound   56.983954\n",
       "20              almost   56.643190\n",
       "21               first   55.468241\n",
       "22       #hurrcaneirma   54.704836\n",
       "23            category   54.704270\n",
       "24               worst   53.554348\n",
       "25           #irma2017   52.584605\n",
       "26       #hurricanirma   51.404446\n",
       "27              closer   50.684188\n",
       "28              little   50.556671"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words[related_words.score > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns all the words whose min-maxed score is greater than an integer threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsScoreGreater(score):\n",
    "    return \"|\".join(list(related_words[related_words.iloc[:,1] > score].iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_encoded[tweet_encoded.tweet_text.str.contains(WordsScoreGreater(70),flags=re.IGNORECASE)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
