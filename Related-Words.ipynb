{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring changes in related words over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will show how words related to a particular word will change over time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import ceil\n",
    "import string\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a separate dataframe for just the tweets classified as english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_en = tweet_full[tweet_full['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing a custom text cleaner. Currently configured to remove all punctuation, _except #_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "#     takes input string and converts or removes characters depending on settings.\n",
    "#     returns a string\n",
    "#     convert case:\n",
    "    tweet = tweet.lower()\n",
    "#     remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "#     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "#     remove punctuation, but not hashtags:\n",
    "    tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('#\\B','',tweet)\n",
    "#     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "#     remove punctuation, including hashtags:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation))\n",
    "#     drop numbers and words of < 4 characters.\n",
    "    tweet = re.sub(r'\\b\\w{1,3}\\b','',tweet)\n",
    "    tweet = re.sub(r'\\b\\d+\\b','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_as_tokens = tweet_tokenizer.tokenize(cleaned_tweet)\n",
    "    tweet_no_stops = [word for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "['omggg', 'hurricane', 'irma', 'sooo', 'much', 'rain', 'cant', 'believe', 'water', 'everywhere']\n"
     ]
    }
   ],
   "source": [
    "re_text = \"this is ! A TWEETlt withgtamp @some 3445 as .@random the amp gt lt @@extra #stuff ##in IT!?@>#! \"\n",
    "re_text = \"OMGGGG @username @username2 Hurricane Irma!!!!!!!! SOOOOO much rain!!!!! I can't believe all of this water everywhere!\"\n",
    "print(len(re_text))\n",
    "print(tokens_no_stopwords(re_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the word we're comparing similarity to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"irma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting here, begin the iteration over times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_words = 200 # number of words to include in cosine similarity ordered list\n",
    "pct_occ_thresh = .01 # words must occur a number of times >= this percent of number of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words from this time frame, based upon the occurrence threshold above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_window = tweet_full_en.loc[tweet_date:tweet_date+date_delta]\n",
    "num_tweets = len(tweet_window)\n",
    "min_count = ceil(num_tweets * pct_occ_thresh)\n",
    "tweet_words = tweet_window['tweet_text'].apply(tokens_no_stopwords)\n",
    "word_counts = tweet_words.apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_window.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_window.loc['2017-09-12 00:00:28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram of word counts. Vertical line represents 1% threshold of word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.hist(word_counts[word_counts > min_count], bins=55,rwidth=.85);\n",
    "plt.axvline(min_count,color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts > min_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats on tokenized tweets. Starting with histogram of tweet length by word count, after processing into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_window['tweet_text'].head(20).apply(tokens_no_stopwords).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig2 = plt.figure(figsize=(12,8))\n",
    "plt.hist(tweet_window['tweet_text'].apply(tokens_no_stopwords).apply(len),bins=22,rwidth=.85);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently there is an incompatibility between gensim and numpy > 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for tweet_day in pd.date_range(start = tweet_date, end = end_date, freq = date_delta):\n",
    "    tweet_text = tweet_full_en.loc[tweet_day:tweet_day + date_delta,\"tweet_text\"]\n",
    "#     min_count = ceil(len(tweet_text) * pct_occ_thresh)\n",
    "    min_count = 1\n",
    "#     this line is just here for diagnostic purposes.\n",
    "#     print(str(tweet_day)+\": \"+str(len(tweet_text))+\" tweets (\"+str(min_count)+\" occurrence threshold)\") \n",
    "\n",
    "    tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=min_count, sg=1, window=3, workers=5, size=100)\n",
    "    word_matrix = vector_model.wv[vector_model.wv.vocab]\n",
    "#     tsne = TSNE(n_components=2)\n",
    "#     result = tsne.fit_transform(word_matrix)\n",
    "#     pca = PCA(n_components=2)\n",
    "#     result = pca.fit_transform(word_matrix)\n",
    "\n",
    "    terms_from_range = pd.DataFrame.from_records(vector_model.wv.most_similar(search_term,topn=top_num_words),columns=[tweet_day,\"Cos_Sim\"])\n",
    "#     query_terms = list(terms_from_range.iloc[:,0])\n",
    "#     query_terms.append(\"irma\")\n",
    "#     print(str(tweet_day))\n",
    "#     print(\" \\'\"+search_term+\"\\' tweets: \"+str(len(tweet_text[tweet_text.str.contains(\"irma\",flags=re.IGNORECASE)])))\n",
    "#     print(len(tweet_text[tweet_text.str.contains(\"|\".join(query_terms),flags=re.IGNORECASE)]))\n",
    "#     print(len(tweet_text))\n",
    "#     filename = str(tweet_day.date())+\".csv\"\n",
    "#     tweet_text[tweet_text.str.contains(\"|\".join(query_terms),flags=re.IGNORECASE)].to_csv(path=filename)\n",
    "    related_words = pd.concat([related_words,terms_from_range],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms = list(terms_from_range.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms.append(\"irma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(terms_from_range.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(list(terms_from_range.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model.wv.similarity(\"irma\",\"storm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(vector_model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model.wv.get_vector('irma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.get_vector(\"storm\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.similarity(\"storm\",\"rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = list(related_words.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.IGNORECASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"|\".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_text[tweet_text.str.contains(\"irma\",flags=re.IGNORECASE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_text[tweet_text.str.contains(\"|\".join(terms),flags=re.IGNORECASE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words.iloc[:,0::2].to_csv(r'daily_words2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bdamage\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bhelping\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing words to hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tweet_window.tweet_text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_count = word_list[word_list.index.str[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_words = hashtags_count.index.str[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list['#hurricaneirma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list[word_list.index.str[1]=='@'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list[hashtag_words].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = list(combinations(list(vector_model.wv.vocab.keys()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in word_pairs:\n",
    "    edge_weight = vector_model.wv.similarity(pair[0],pair[1])\n",
    "    if edge_weight > .80:\n",
    "        tweet_graph.add_edge(pair[0],pair[1],weight=edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_graph.add_nodes_from(vector_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(tweet_graph,path=r'./tweet_graph.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
