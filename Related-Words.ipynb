{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring changes in related words over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will show how words related to a particular word will change over time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from math import ceil\n",
    "import string\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a separate dataframe for just the tweets classified as english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_en = tweet_full[tweet_full['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing a custom text cleaner. Currently configured to remove all punctuation, _except #_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "#     takes input string and converts or removes characters depending on settings.\n",
    "#     returns a string\n",
    "#     convert case:\n",
    "    tweet = tweet.lower()\n",
    "#     remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "#     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "#     remove punctuation, but not hashtags:\n",
    "    tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('#\\B','',tweet)\n",
    "#     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "#     remove punctuation, including hashtags:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_as_tokens = tweet_tokenizer.tokenize(cleaned_tweet)\n",
    "    tweet_no_stops = [word for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweetlt', 'withgtamp', '#stuff', '#in']\n"
     ]
    }
   ],
   "source": [
    "re_text = \"this is ! A TWEETlt withgtamp @some .@random amp gt lt @@extra #stuff ##in IT!?@>#! \"\n",
    "print(tokens_no_stopwords(re_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "florida                1774\n",
       "#hurricaneirma         1658\n",
       "reports                1385\n",
       "hurricane              1316\n",
       "irma                   1313\n",
       "#irma                  1250\n",
       "fl                     1232\n",
       "mph                    1169\n",
       "asos                   1118\n",
       "gust                   1076\n",
       "knots                  1027\n",
       "power                  1024\n",
       "storm                   726\n",
       "wind                    710\n",
       "rain                    661\n",
       "still                   605\n",
       "safe                    571\n",
       "like                    562\n",
       "beach                   542\n",
       "county                  534\n",
       "get                     492\n",
       "us                      425\n",
       "go                      422\n",
       "miami                   421\n",
       "good                    401\n",
       "pm                      397\n",
       "f                       386\n",
       "right                   385\n",
       "stay                    360\n",
       "co                      355\n",
       "                       ... \n",
       "basics                    1\n",
       "restbetter                1\n",
       "#carpool                  1\n",
       "aswell                    1\n",
       "stressbut                 1\n",
       "73400                     1\n",
       "dancehall                 1\n",
       "dulcet                    1\n",
       "#cocktail                 1\n",
       "#staysafeandhealthy       1\n",
       "alaska                    1\n",
       "1b                        1\n",
       "dioxide                   1\n",
       "#cowboys                  1\n",
       "tilting                   1\n",
       "#thedeuce                 1\n",
       "statement                 1\n",
       "mentally                  1\n",
       "diz                       1\n",
       "lag                       1\n",
       "06521                     1\n",
       "briefings                 1\n",
       "0636z                     1\n",
       "wales                     1\n",
       "knoll                     1\n",
       "graham                    1\n",
       "storage                   1\n",
       "dole                      1\n",
       "#staugustinebeach         1\n",
       "#waterytart               1\n",
       "Length: 18534, dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_full_en.loc[\"2017-09-10 09:00:00\":\"2017-09-11 09:00:00\",'tweet_text'].apply(tokens_no_stopwords).apply(pd.Series).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the word we're comparing similarity to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"irma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting here, begin the iteration over times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_words = 20 # number of words to include in cosine similarity ordered list\n",
    "pct_occ_thresh = .01 # words must occur a number of times >= this percent of number of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words from this time frame, based upon the occurrence threshold above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "florida           1774\n",
       "#hurricaneirma    1658\n",
       "reports           1385\n",
       "hurricane         1316\n",
       "irma              1313\n",
       "#irma             1250\n",
       "fl                1232\n",
       "mph               1169\n",
       "asos              1118\n",
       "gust              1076\n",
       "knots             1027\n",
       "power             1024\n",
       "storm              726\n",
       "wind               710\n",
       "rain               661\n",
       "still              605\n",
       "safe               571\n",
       "like               562\n",
       "beach              542\n",
       "county             534\n",
       "get                492\n",
       "us                 425\n",
       "go                 422\n",
       "miami              421\n",
       "good               401\n",
       "pm                 397\n",
       "f                  386\n",
       "right              385\n",
       "stay               360\n",
       "co                 355\n",
       "                  ... \n",
       "day                254\n",
       "ese                249\n",
       "st                 248\n",
       "know               241\n",
       "2                  237\n",
       "shit               236\n",
       "outside            228\n",
       "weather            226\n",
       "palm               226\n",
       "west               226\n",
       "humidity           219\n",
       "edt                218\n",
       "tampa              217\n",
       "little             214\n",
       "#hurrcaneirma      209\n",
       "water              208\n",
       "fort               207\n",
       "#miami             206\n",
       "eye                205\n",
       "1                  205\n",
       "really             203\n",
       "home               203\n",
       "lol                202\n",
       "need               198\n",
       "thanks             197\n",
       "god                197\n",
       "please             194\n",
       "even               193\n",
       "well               193\n",
       "current            192\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tweets = len(tweet_full[tweet_full.tweet_lang == 'en'].loc[tweet_date:tweet_date+date_delta])\n",
    "min_count = ceil(num_tweets * pct_occ_thresh)\n",
    "tweet_words = tweet_full[tweet_full.tweet_lang == 'en'].loc[\"2017-09-10 09:00:00\":\"2017-09-11 09:00:00\",'tweet_text'].apply(tokens_no_stopwords)\n",
    "word_counts = tweet_words.apply(pd.Series).stack().value_counts()\n",
    "word_counts[word_counts > min_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words per tweet, after processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2017-09-10 09:00:01    [wind, 10, mph, nne, barometer, 29832, falling...\n",
       "2017-09-10 09:00:03    [im, going, live, national, fox, news, fews, m...\n",
       "2017-09-10 09:00:07                   [hear, shutters, shake, one, time]\n",
       "2017-09-10 09:00:11    [0457, temp, 783, f, hum, 92, dewp, 752, f, ba...\n",
       "2017-09-10 09:00:20                                        [#irma, woke]\n",
       "2017-09-10 09:00:29                                  [still, awake, lol]\n",
       "2017-09-10 09:00:50    [reagan, running, lebanon, marine, barracks, b...\n",
       "2017-09-10 09:00:53    [please, pray, florida, keys, keep, talking, s...\n",
       "2017-09-10 09:01:29                                     [great, sweetie]\n",
       "2017-09-10 09:02:04    [454, 1, w, coral, springs, broward, co, fl, m...\n",
       "2017-09-10 09:02:08    [barometric, pressure, plummeting, fast, 973mb...\n",
       "2017-09-10 09:02:10      [irma, eye, move, across, lower, florida, keys]\n",
       "2017-09-10 09:02:18                                              [karma]\n",
       "2017-09-10 09:02:21    [#firstresponders, everyday, heros, right, mid...\n",
       "2017-09-10 09:02:21                [sour, grapes, hillary, sour, grapes]\n",
       "2017-09-10 09:02:41                                    [happy, birthday]\n",
       "2017-09-10 09:03:22                    [sounds, like, monsters, outside]\n",
       "2017-09-10 09:03:23                                               [amen]\n",
       "2017-09-10 09:03:23                           [live, state, pool, party]\n",
       "2017-09-10 09:03:42    [#hurricaneirma, set, make, landfall, #florida...\n",
       "2017-09-10 09:03:43    [h2bn, fam, bcurtis, 26, chilling, queen, godd...\n",
       "2017-09-10 09:03:48      [irma, 5, 40, miles, southsoutheast, key, west]\n",
       "2017-09-10 09:03:49                                         [nightmares]\n",
       "2017-09-10 09:04:03    [overnight, irma, met, 130, mph, threshold, ca...\n",
       "2017-09-10 09:04:11           [sleep, shit, irma, going, crazy, outside]\n",
       "2017-09-10 09:04:27                                        [power, went]\n",
       "2017-09-10 09:04:46                                      [fux, wit, joe]\n",
       "2017-09-10 09:04:50                           [let, catch, beauty, rest]\n",
       "2017-09-10 09:05:14                          [fine, lauderhill, florida]\n",
       "2017-09-10 09:05:28    [bruh, #hurricaineirma, sucked, beach, water, ...\n",
       "                                             ...                        \n",
       "2017-09-11 08:52:41    [earlier, around, 2am, coming, back, walk, che...\n",
       "2017-09-11 08:52:42    [see, irma, knew, tree, gon, fall, im, thankfu...\n",
       "2017-09-11 08:52:47                     [posted, photo, caribbean, isle]\n",
       "2017-09-11 08:53:55                        [night, possibly, get, worse]\n",
       "2017-09-11 08:54:10    [200, pine, hills, orange, co, fl, fire, deptr...\n",
       "2017-09-11 08:54:22                  [yaaa, hurricane, irma, gone, rain]\n",
       "2017-09-11 08:54:32        [shout, everything, went, still, heart, gold]\n",
       "2017-09-11 08:55:04    [#naturalremedies, see, happens, body, eat, 3,...\n",
       "2017-09-11 08:55:34                                       [wind, insane]\n",
       "2017-09-11 08:55:42    [house, street, eye, irma, came, right, us, pr...\n",
       "2017-09-11 08:56:03    [openings, today, text, number, bio, #thegrind...\n",
       "2017-09-11 08:56:04    [jacksonvillecraigfl, crg, asos, reports, gust...\n",
       "2017-09-11 08:56:04    [orlandoherndonfl, orl, asos, reports, gust, 6...\n",
       "2017-09-11 08:56:08                                          [seriously]\n",
       "2017-09-11 08:56:08    [leesburg, municipal, airportfl, lee, asos, re...\n",
       "2017-09-11 08:56:08    [daytona, beach, rgnlfl, dab, asos, reports, g...\n",
       "2017-09-11 08:56:08    [orlando, jetportfl, mco, asos, reports, gust,...\n",
       "2017-09-11 08:56:57    [love, sleeping, ima, pull, nighters, like, 3,...\n",
       "2017-09-11 08:56:59    [trees, branches, knocked, lot, floods, places...\n",
       "2017-09-11 08:57:12    [451am, et, irma, long, gone, handed, controls...\n",
       "2017-09-11 08:57:19    [#naturalremedies, 5, home, remedies, treating...\n",
       "2017-09-11 08:57:25                        [floor, 37, million, dollars]\n",
       "2017-09-11 08:57:39    [#haveahartchallenge, auntie, look, lol, duval...\n",
       "2017-09-11 08:57:48                                      [tired, snacks]\n",
       "2017-09-11 08:57:58    [getting, ready, next, round, #irma, storm, joke]\n",
       "2017-09-11 08:58:49    [hurricane, irma, 911, attacks, two, catastrop...\n",
       "2017-09-11 08:58:59    [443, tallahassee, regional, leon, co, fl, aso...\n",
       "2017-09-11 08:59:18    [heavy, intensity, rain, light, rain, temperat...\n",
       "2017-09-11 08:59:45                            [crazy, night, exhausted]\n",
       "2017-09-11 09:00:00                                 [kind, people, need]\n",
       "Name: tweet_text, Length: 18523, dtype: object"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_full[tweet_full.tweet_lang == 'en'].loc[\"2017-09-10 09:00:00\":\"2017-09-11 09:00:00\",'tweet_text'].apply(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently there is an incompatibility between gensim and numpy > 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for tweet_day in pd.date_range(start = tweet_date, end = end_date, freq = date_delta):\n",
    "    tweet_text = tweet_full[tweet_full.tweet_lang == 'en'].loc[tweet_day:tweet_day + date_delta,\"tweet_text\"]\n",
    "    min_count = ceil(len(tweet_text) * pct_occ_thresh)\n",
    "#     this line is just here for diagnostic purposes.\n",
    "#     print(str(tweet_day)+\": \"+str(len(tweet_text))+\" tweets (\"+str(min_count)+\" occurrence threshold)\") \n",
    "\n",
    "    tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=min_count, sg=1, window=3, workers=5, size=100)\n",
    "    word_matrix = vector_model.wv[vector_model.wv.vocab]\n",
    "#     tsne = TSNE(n_components=2)\n",
    "#     result = tsne.fit_transform(word_matrix)\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_matrix)\n",
    "\n",
    "    terms_from_range = pd.DataFrame.from_records(vector_model.wv.most_similar(search_term,topn=top_num_words),columns=[tweet_day,\"Cos_Sim\"])\n",
    "    related_words = pd.concat([related_words,terms_from_range],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.get_vector(\"storm\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.similarity(\"storm\",\"rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "florida           2257\n",
       "#hurricaneirma    1980\n",
       "#irma             1585\n",
       "irma              1537\n",
       "reports           1387\n",
       "hurricane         1344\n",
       "fl                1295\n",
       "mph               1177\n",
       "asos              1118\n",
       "gust              1076\n",
       "power             1030\n",
       "knots             1027\n",
       "de                 749\n",
       "storm              731\n",
       "wind               714\n",
       "miami              684\n",
       "rain               662\n",
       "beach              621\n",
       "still              607\n",
       "safe               575\n",
       "en                 568\n",
       "like               565\n",
       "county             553\n",
       "get                493\n",
       "n                  467\n",
       "us                 427\n",
       "pm                 426\n",
       "go                 426\n",
       "good               402\n",
       "#florida           398\n",
       "                  ... \n",
       "winds              354\n",
       "back               350\n",
       "que                349\n",
       "going              346\n",
       "#hurricane         344\n",
       "got                343\n",
       "house              339\n",
       "time               330\n",
       "people             320\n",
       "la                 316\n",
       "one                314\n",
       "se                 314\n",
       "getting            301\n",
       "love               291\n",
       "everyone           286\n",
       "today              286\n",
       "update             283\n",
       "warning            277\n",
       "2                  266\n",
       "#hurrcaneirma      262\n",
       "st                 259\n",
       "ese                256\n",
       "day                256\n",
       "see                256\n",
       "thank              255\n",
       "west               243\n",
       "know               241\n",
       "palm               238\n",
       "tampa              238\n",
       "shit               236\n",
       "Length: 69, dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts > min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#irma</td>\n",
       "      <td>#irma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>florida</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#florida</td>\n",
       "      <td>#hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#hurricane</td>\n",
       "      <td>everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#miami</td>\n",
       "      <td>#hurricaneirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stay</td>\n",
       "      <td>#florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>county</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>us</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#hurricaneirma</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>miami</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>safe</td>\n",
       "      <td>power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hurricane</td>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doral</td>\n",
       "      <td>please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>like</td>\n",
       "      <td>getting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>getting</td>\n",
       "      <td>#miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>update</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>power</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>right</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>everyone</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>house</td>\n",
       "      <td>florida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-10 00:00:00 2017-09-10 00:00:00\n",
       "0                #irma               #irma\n",
       "1              florida                stay\n",
       "2             #florida          #hurricane\n",
       "3           #hurricane            everyone\n",
       "4               #miami      #hurricaneirma\n",
       "5                 stay            #florida\n",
       "6               county                safe\n",
       "7                   us               house\n",
       "8       #hurricaneirma                  us\n",
       "9                miami                like\n",
       "10                safe               power\n",
       "11           hurricane               still\n",
       "12               doral              please\n",
       "13                like             getting\n",
       "14             getting              #miami\n",
       "15              update                 get\n",
       "16               power                 day\n",
       "17               right                 got\n",
       "18            everyone               going\n",
       "19               house             florida"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words.iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bdamage\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bhelping\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing words to hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2017-09-01 02:42:56    @matt_swag1  amp  _its_guwap TURNIN UP TO MY #...\n",
       "2017-09-01 03:19:14    being bae today and running errands with her w...\n",
       "2017-09-01 04:24:25    FULL VIDEO IN MY BIO  lt ---          #Benzo o...\n",
       "2017-09-01 04:49:28    Summer 1997  lt  Summer 2017  minus the Nazi s...\n",
       "2017-09-01 08:16:10    In my head  lt 3 en I m Not Lost I m RVing htt...\n",
       "2017-09-01 09:01:16    In my head  lt 3 en I m Not Lost I m RVing htt...\n",
       "2017-09-01 13:54:16                   introducing yourself in class  lt \n",
       "2017-09-01 14:59:08    Reminder:   gt  gt  Labor Day is on Monday, Se...\n",
       "2017-09-01 15:45:23    The Danger that Lies Within  Diet  Foods... ht...\n",
       "2017-09-01 16:53:56    John Kelly considered resigning after Comey wa...\n",
       "2017-09-01 17:53:34    @FoxBusiness @CharlesHurt Straight-thinking Ch...\n",
       "2017-09-01 22:17:37    E d_Mobarek to all the muselmans poeple  lt 3 ...\n",
       "2017-09-02 15:06:57    When your favorite bartender isnt in the clubh...\n",
       "2017-09-02 17:39:17    i just saw this im such a great person. ily  l...\n",
       "2017-09-02 18:46:03    bringing the fall season in early with dollywa...\n",
       "2017-09-02 19:39:54     lt  lt Smooth Cri N@L gt  gt  i ll getchya 9 ...\n",
       "2017-09-02 20:08:37              @realmandyrain 2 Beautiful Ladys   lt 3\n",
       "2017-09-02 22:05:53           @lindeelink U look Beyond Beautiful   lt 3\n",
       "2017-09-03 01:40:08    Suprised this beautiful girl today  lt 3 https...\n",
       "2017-09-03 02:04:38    @nafiakhan1 You are glowing, mA     lt 3 So ex...\n",
       "2017-09-03 02:13:20                  Word.  lt 3 https://t.co/lsgsoxRaKX\n",
       "2017-09-03 02:34:29    No lie, @TheFieryFox makes the best bak lava h...\n",
       "2017-09-03 03:30:40     @Larkawuff OMG please get em. We can match  lt 3\n",
       "2017-09-03 07:08:00    Finding out things I never wanted to know lt  ...\n",
       "2017-09-03 15:19:11    @mattrosell Im crying in the club right now  l...\n",
       "2017-09-03 15:21:23    When in doubt, hype it up   lt /sarcasm gt  ht...\n",
       "2017-09-03 16:28:12    .@DWStweets .@RepDWStweets   lt    #LockHerUp ...\n",
       "2017-09-03 16:30:49    @meanpIastic Very cool but I could never ride ...\n",
       "2017-09-03 17:34:49    @a40OZofKARLiTO Seriously ily keep reminiscing...\n",
       "2017-09-03 18:26:45    Magic Kingdom with my Prince Charming lt 3 #di...\n",
       "                                             ...                        \n",
       "2017-10-07 20:12:23                            @xobabyjo Incorrect  lt 3\n",
       "2017-10-07 21:17:07    Here s how a  fu lt king moron  celebrates His...\n",
       "2017-10-07 23:11:22                         lt 3 https://t.co/S6qsqqqzAg\n",
       "2017-10-07 23:58:01    Dissin the same nigga you smiling in pics with...\n",
       "2017-10-08 03:04:57                   @Skylar_Eventive  lt 3  lt 3  lt 3\n",
       "2017-10-08 12:26:57    @lindeelink Just bought Someday on Amazon Lind...\n",
       "2017-10-08 17:02:17    And Tillerson created  fu lt kin  moron . So m...\n",
       "2017-10-08 17:32:16    @JonnieBaker7 Go back into your hole #TwitterT...\n",
       "2017-10-08 19:19:22     lt  lt  You don t need to be perfect to be go...\n",
       "2017-10-08 19:20:40             @jarvisthecpu Get us matchin pairs  lt 3\n",
       "2017-10-08 19:34:19                   Phat asses  gt   lt  Juicy Titties\n",
       "2017-10-08 21:16:08    Me gustabas pq eras distinto... ahora eres igu...\n",
       "2017-10-09 00:17:01    Supporting local music   :  Hangin  with THIS ...\n",
       "2017-10-09 01:36:38    i love when my friends travel down south to se...\n",
       "2017-10-09 02:41:03    People who only reach out to you when it s con...\n",
       "2017-10-09 04:25:37    ur always confusin sis but  lt 3 u https://t.c...\n",
       "2017-10-09 14:05:04    This is why I hop around to my family member  ...\n",
       "2017-10-09 14:30:35    Let your dreams be bigger then your fears, and...\n",
       "2017-10-09 18:11:42    gettin a good nod in at work   lt  gt  gt  gt ...\n",
       "2017-10-09 20:34:54     lt SWIPE gt  Trophy Wife      #prettyBaby #me...\n",
       "2017-10-09 21:33:01    #GSOTD  #Gym Song of the Day :  I  lt 3 EC2  -...\n",
       "2017-10-10 01:30:43    Sad for my #seminoles it s been a rough year  ...\n",
       "2017-10-10 02:19:56    H A P P Y  B I R T H D A Y   j.j_loretto #brot...\n",
       "2017-10-10 03:28:24    prayers for Christian s family and to those st...\n",
       "2017-10-10 09:57:38    @corbynbesson November 8 is my birthday so can...\n",
       "2017-10-10 10:37:00                         lt 3 https://t.co/cYlXAT1b1Z\n",
       "2017-10-10 14:19:08    A baby is a beautiful blessing  lt 3 With Jess...\n",
       "2017-10-10 16:39:08    Los dramas y griter os de toda mi familia junt...\n",
       "2017-10-10 20:45:06     @Impeach_D_Trump Stand by your fu lt king moron.\n",
       "2017-10-10 23:51:51                       @digamecampos mucho amor  lt 3\n",
       "Name: tweet_text, Length: 465, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_full[tweet_full.tweet_text.str.contains(\"\\\\blt\\\\b\")].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tweet_full.loc[\"2017-09-10 09:00:00\":\"2017-09-11 09:00:00\"].tweet_text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_count = word_list[word_list.index.str[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_words = hashtags_count.index.str[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the                        5790\n",
       "@                          4370\n",
       "i                          4057\n",
       "to                         3717\n",
       "a                          3586\n",
       "of                         3394\n",
       "in                         3178\n",
       "and                        3034\n",
       "                           2811\n",
       "is                         2761\n",
       "my                         2334\n",
       "florida                    2124\n",
       "this                       2011\n",
       "s                          1948\n",
       "#hurricaneirma             1913\n",
       "for                        1859\n",
       "from                       1806\n",
       "you                        1741\n",
       "it                         1612\n",
       "we                         1607\n",
       "on                         1543\n",
       "#irma                      1527\n",
       "at                         1505\n",
       "reports                    1386\n",
       "t                          1351\n",
       "irma                       1289\n",
       "hurricane                  1257\n",
       "are                        1166\n",
       "mph                        1160\n",
       "asos                       1118\n",
       "                           ... \n",
       "season...                     1\n",
       "ndolos                        1\n",
       "@wnba                         1\n",
       "grrr                          1\n",
       "14:57                         1\n",
       "hawk                          1\n",
       "@miamidolphins                1\n",
       "finn                          1\n",
       "#palmaire                     1\n",
       "https://t.co/2xoetp7r1r       1\n",
       "https://t.co/bemffcoscq       1\n",
       "https://t.co/kfhv9tjues       1\n",
       "https://t.co/lytyczbpuh       1\n",
       "https://t.co/ldtx3mnyd6       1\n",
       "https://t.co/kpv3eoycbl       1\n",
       "https://t.co/dfbi3dlxdj       1\n",
       "#teelingwhiskey               1\n",
       "https://t.co/o0evrh0jzw       1\n",
       "https://t.co/6dzpwf4uj0       1\n",
       "https://t.co/3nrxtbagpn       1\n",
       "tanto,                        1\n",
       "https://t.co/dwxaxcyxqw       1\n",
       "stang                         1\n",
       "tomada                        1\n",
       "#interconmiami                1\n",
       "loooud                        1\n",
       "4yr                           1\n",
       "town....                      1\n",
       "https://t.co/aggs23oduy       1\n",
       "umbrella.                     1\n",
       "Length: 45446, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1913"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list['#hurricaneirma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['.@mayorgimenez', '.@cbs12', '.@flashgjr', '.@10newswtsp',\n",
       "       '.@richarddymond', '.@drtiajolie', '#@abc', '.@realdonaldtrump',\n",
       "       '.@andrewwulfeck:', '.@occc', '.@rborn83,', '.@miamidadecounty',\n",
       "       '.@manateesheriff', '.@deadpool1973', '-@notcampbellmatt',\n",
       "       '.@dukeenergy', '.@miamidadefire', '-@grant_gilmore', '.@jimsmallman',\n",
       "       '.@thecwsupergirl', 'w@30.', '.@goabode', 'l@s', '.@jason_lanning',\n",
       "       '.@tampaelectric', '.@nicoleebryan', '.@potus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[word_list.index.str[1]=='@'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   2811.0\n",
       "my                                 2334.0\n",
       "florida                            2124.0\n",
       "this                               2011.0\n",
       "you                                1741.0\n",
       "it                                 1612.0\n",
       "we                                 1607.0\n",
       "irma                               1289.0\n",
       "hurricane                          1257.0\n",
       "gust                               1074.0\n",
       "me                                  952.0\n",
       "power                               878.0\n",
       "fl                                  847.0\n",
       "wind                                656.0\n",
       "storm                               630.0\n",
       "our                                 563.0\n",
       "now                                 524.0\n",
       "rain                                499.0\n",
       "down                                477.0\n",
       "safe                                471.0\n",
       "miami                               468.0\n",
       "go                                  410.0\n",
       "will                                393.0\n",
       "beach                               384.0\n",
       "stay                                360.0\n",
       "tornado                             357.0\n",
       "us                                  345.0\n",
       "got                                 343.0\n",
       "winds                               317.0\n",
       "f                                   313.0\n",
       "                                    ...  \n",
       "fortmyers.                            NaN\n",
       "9september                            NaN\n",
       "doglife                               NaN\n",
       "piperlove                             NaN\n",
       "russiansinflorida                     NaN\n",
       "staybless                             NaN\n",
       "pawpaw                                NaN\n",
       "illinois                              NaN\n",
       "fortpierce,                           NaN\n",
       "uracanirma                            NaN\n",
       "btwwearesafe                          NaN\n",
       "iwon                                  NaN\n",
       "laryngitis#hurricaneirma#losttv       NaN\n",
       "mydailyinspiration                    NaN\n",
       "hotterthanthedevilsdick               NaN\n",
       "vibrantatanyage                       NaN\n",
       "ihateyouirma                          NaN\n",
       "mantra                                NaN\n",
       "itshappening                          NaN\n",
       "cholanews                             NaN\n",
       "gobucs                                NaN\n",
       "emcee                                 NaN\n",
       "expeditioneverest                     NaN\n",
       "loveislove                            NaN\n",
       "bibliotherapy                         NaN\n",
       "irma.ahora                            NaN\n",
       "lonelyplanet                          NaN\n",
       "palmaire                              NaN\n",
       "teelingwhiskey                        NaN\n",
       "interconmiami                         NaN\n",
       "Length: 4690, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[hashtag_words].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = list(combinations(list(vector_model.wv.vocab.keys()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in word_pairs:\n",
    "    edge_weight = vector_model.wv.similarity(pair[0],pair[1])\n",
    "    if edge_weight > .80:\n",
    "        tweet_graph.add_edge(pair[0],pair[1],weight=edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_graph.add_nodes_from(vector_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(tweet_graph,path=r'./tweet_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
