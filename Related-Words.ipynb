{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring changes in related words over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will show how words related to a particular word will change over time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from math import ceil\n",
    "import string\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing a custom text cleaner. Currently configured to remove all punctuation, _except #_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "#     convert case:\n",
    "    tweet = tweet.lower()\n",
    "#     remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "#     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "#     remove punctuation, but not hashtags:\n",
    "    tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('#\\B','',tweet)\n",
    "#     remove 'amp', 'gt', 'lt', indicating decoded ampersand, greater-than, less-than characters\n",
    "    tweet = re.sub(r'\\b(amp|gt|lt)\\b','',tweet)\n",
    "#     remove punctuation, including hashtags:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "def tokens_no_stopwords(tweet_as_string):\n",
    "#     wrapper function that combines the tokenizer, cleaner, and stopword removal.\n",
    "#     takes a string and returns a list of strings\n",
    "    cleaned_tweet = clean_tweet(tweet_as_string)\n",
    "    tweet_as_tokens = tweet_tokenizer.tokenize(cleaned_tweet)\n",
    "    tweet_no_stops = [word for word in tweet_as_tokens if word not in tweet_stops]\n",
    "    \n",
    "    return tweet_no_stops\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweetlt', 'withgtamp', '#stuff', '#in']\n"
     ]
    }
   ],
   "source": [
    "re_text = \"this is ! A TWEETlt withgtamp @some .@random amp gt lt @@extra #stuff ##in IT!?@>#! \"\n",
    "print(tokens_no_stopwords(re_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the word we're comparing similarity to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"irma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting here, begin the iteration over times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-10 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-10 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_words = 20 # number of words to include in cosine similarity ordered list\n",
    "pct_occ_thresh = .01 # words must occur a number of times >= this percent of number of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently there is an incompatibility between gensim and numpy > 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-10 00:00:00: 22953 tweets (230 occurrence threshold)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for tweet_day in pd.date_range(start = tweet_date, end = end_date, freq = date_delta):\n",
    "    \n",
    "    tweet_text = tweet_full.loc[tweet_day:tweet_day + date_delta,\"tweet_text\"]\n",
    "    min_count = ceil(len(tweet_text) * pct_occ_thresh)\n",
    "    print(str(tweet_day)+\": \"+str(len(tweet_text))+\" tweets (\"+str(min_count)+\" occurrence threshold)\") # this line is just here for diagnostic purposes.\n",
    "    \n",
    "    tweets_tokens = tweet_text.apply(tokens_no_stopwords)\n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=min_count, sg=3, window=1)\n",
    "    word_matrix = vector_model.wv[vector_model.wv.vocab]\n",
    "#     tsne = TSNE(n_components=2)\n",
    "#     result = tsne.fit_transform(word_matrix)\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_matrix)\n",
    "    \n",
    "    terms_from_range = pd.DataFrame.from_records(vector_model.wv.most_similar(search_term,topn=top_num_words),columns=[tweet_day,\"Cos_Sim\"])\n",
    "    related_words = pd.concat([related_words,terms_from_range],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.get_vector(\"storm\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.similarity(\"storm\",\"rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "      <th>Cos_Sim</th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "      <th>Cos_Sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ready</td>\n",
       "      <td>0.730827</td>\n",
       "      <td>#hurricaneirma</td>\n",
       "      <td>0.961513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#hurricaineirma</td>\n",
       "      <td>0.721239</td>\n",
       "      <td>#irma</td>\n",
       "      <td>0.955641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coverage</td>\n",
       "      <td>0.718990</td>\n",
       "      <td>house</td>\n",
       "      <td>0.954888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>waiting</td>\n",
       "      <td>0.714566</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.933836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pass</td>\n",
       "      <td>0.712750</td>\n",
       "      <td>update</td>\n",
       "      <td>0.928950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.712392</td>\n",
       "      <td>#hurricane</td>\n",
       "      <td>0.926753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>party</td>\n",
       "      <td>0.712297</td>\n",
       "      <td>us</td>\n",
       "      <td>0.924933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hit</td>\n",
       "      <td>0.711810</td>\n",
       "      <td>need</td>\n",
       "      <td>0.924002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#hurricanirma</td>\n",
       "      <td>0.709186</td>\n",
       "      <td>shit</td>\n",
       "      <td>0.923062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>live</td>\n",
       "      <td>0.706116</td>\n",
       "      <td>got</td>\n",
       "      <td>0.922459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>latest</td>\n",
       "      <td>0.705344</td>\n",
       "      <td>time</td>\n",
       "      <td>0.920123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.703610</td>\n",
       "      <td>people</td>\n",
       "      <td>0.919905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>beer</td>\n",
       "      <td>0.701862</td>\n",
       "      <td>right</td>\n",
       "      <td>0.919481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shot</td>\n",
       "      <td>0.701845</td>\n",
       "      <td>love</td>\n",
       "      <td>0.917609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hits</td>\n",
       "      <td>0.701467</td>\n",
       "      <td>everyone</td>\n",
       "      <td>0.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>path</td>\n",
       "      <td>0.700955</td>\n",
       "      <td>#florida</td>\n",
       "      <td>0.916754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#huricaneirma</td>\n",
       "      <td>0.700509</td>\n",
       "      <td>thank</td>\n",
       "      <td>0.915861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>drive</td>\n",
       "      <td>0.700381</td>\n",
       "      <td>back</td>\n",
       "      <td>0.913342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#goawayirma</td>\n",
       "      <td>0.699328</td>\n",
       "      <td>going</td>\n",
       "      <td>0.907873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>making</td>\n",
       "      <td>0.698440</td>\n",
       "      <td>go</td>\n",
       "      <td>0.906146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-10 00:00:00   Cos_Sim 2017-09-10 00:00:00   Cos_Sim\n",
       "0                ready  0.730827      #hurricaneirma  0.961513\n",
       "1      #hurricaineirma  0.721239               #irma  0.955641\n",
       "2             coverage  0.718990               house  0.954888\n",
       "3              waiting  0.714566                 lol  0.933836\n",
       "4                 pass  0.712750              update  0.928950\n",
       "5                    4  0.712392          #hurricane  0.926753\n",
       "6                party  0.712297                  us  0.924933\n",
       "7                  hit  0.711810                need  0.924002\n",
       "8        #hurricanirma  0.709186                shit  0.923062\n",
       "9                 live  0.706116                 got  0.922459\n",
       "10              latest  0.705344                time  0.920123\n",
       "11                2017  0.703610              people  0.919905\n",
       "12                beer  0.701862               right  0.919481\n",
       "13                shot  0.701845                love  0.917609\n",
       "14                hits  0.701467            everyone  0.916800\n",
       "15                path  0.700955            #florida  0.916754\n",
       "16       #huricaneirma  0.700509               thank  0.915861\n",
       "17               drive  0.700381                back  0.913342\n",
       "18         #goawayirma  0.699328               going  0.907873\n",
       "19              making  0.698440                  go  0.906146"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-10 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#hurricaineirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#hurricanirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>beer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#huricaneirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-10 00:00:00\n",
       "0                ready\n",
       "1             coverage\n",
       "2      #hurricaineirma\n",
       "3                party\n",
       "4        #hurricanirma\n",
       "5                 pass\n",
       "6                  hit\n",
       "7                 live\n",
       "8              waiting\n",
       "9                 2017\n",
       "10                path\n",
       "11                hits\n",
       "12              normal\n",
       "13               drive\n",
       "14              making\n",
       "15              snacks\n",
       "16                beer\n",
       "17       #huricaneirma\n",
       "18                    \n",
       "19              latest"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words.iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bdamage\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bhelping\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing words to hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2017-09-01 02:42:56    @matt_swag1  amp  _its_guwap TURNIN UP TO MY #...\n",
       "2017-09-01 03:19:14    being bae today and running errands with her w...\n",
       "2017-09-01 04:24:25    FULL VIDEO IN MY BIO  lt ---          #Benzo o...\n",
       "2017-09-01 04:49:28    Summer 1997  lt  Summer 2017  minus the Nazi s...\n",
       "2017-09-01 08:16:10    In my head  lt 3 en I m Not Lost I m RVing htt...\n",
       "2017-09-01 09:01:16    In my head  lt 3 en I m Not Lost I m RVing htt...\n",
       "2017-09-01 13:54:16                   introducing yourself in class  lt \n",
       "2017-09-01 14:59:08    Reminder:   gt  gt  Labor Day is on Monday, Se...\n",
       "2017-09-01 15:45:23    The Danger that Lies Within  Diet  Foods... ht...\n",
       "2017-09-01 16:53:56    John Kelly considered resigning after Comey wa...\n",
       "2017-09-01 17:53:34    @FoxBusiness @CharlesHurt Straight-thinking Ch...\n",
       "2017-09-01 22:17:37    E d_Mobarek to all the muselmans poeple  lt 3 ...\n",
       "2017-09-02 15:06:57    When your favorite bartender isnt in the clubh...\n",
       "2017-09-02 17:39:17    i just saw this im such a great person. ily  l...\n",
       "2017-09-02 18:46:03    bringing the fall season in early with dollywa...\n",
       "2017-09-02 19:39:54     lt  lt Smooth Cri N@L gt  gt  i ll getchya 9 ...\n",
       "2017-09-02 20:08:37              @realmandyrain 2 Beautiful Ladys   lt 3\n",
       "2017-09-02 22:05:53           @lindeelink U look Beyond Beautiful   lt 3\n",
       "2017-09-03 01:40:08    Suprised this beautiful girl today  lt 3 https...\n",
       "2017-09-03 02:04:38    @nafiakhan1 You are glowing, mA     lt 3 So ex...\n",
       "2017-09-03 02:13:20                  Word.  lt 3 https://t.co/lsgsoxRaKX\n",
       "2017-09-03 02:34:29    No lie, @TheFieryFox makes the best bak lava h...\n",
       "2017-09-03 03:30:40     @Larkawuff OMG please get em. We can match  lt 3\n",
       "2017-09-03 07:08:00    Finding out things I never wanted to know lt  ...\n",
       "2017-09-03 15:19:11    @mattrosell Im crying in the club right now  l...\n",
       "2017-09-03 15:21:23    When in doubt, hype it up   lt /sarcasm gt  ht...\n",
       "2017-09-03 16:28:12    .@DWStweets .@RepDWStweets   lt    #LockHerUp ...\n",
       "2017-09-03 16:30:49    @meanpIastic Very cool but I could never ride ...\n",
       "2017-09-03 17:34:49    @a40OZofKARLiTO Seriously ily keep reminiscing...\n",
       "2017-09-03 18:26:45    Magic Kingdom with my Prince Charming lt 3 #di...\n",
       "                                             ...                        \n",
       "2017-10-07 20:12:23                            @xobabyjo Incorrect  lt 3\n",
       "2017-10-07 21:17:07    Here s how a  fu lt king moron  celebrates His...\n",
       "2017-10-07 23:11:22                         lt 3 https://t.co/S6qsqqqzAg\n",
       "2017-10-07 23:58:01    Dissin the same nigga you smiling in pics with...\n",
       "2017-10-08 03:04:57                   @Skylar_Eventive  lt 3  lt 3  lt 3\n",
       "2017-10-08 12:26:57    @lindeelink Just bought Someday on Amazon Lind...\n",
       "2017-10-08 17:02:17    And Tillerson created  fu lt kin  moron . So m...\n",
       "2017-10-08 17:32:16    @JonnieBaker7 Go back into your hole #TwitterT...\n",
       "2017-10-08 19:19:22     lt  lt  You don t need to be perfect to be go...\n",
       "2017-10-08 19:20:40             @jarvisthecpu Get us matchin pairs  lt 3\n",
       "2017-10-08 19:34:19                   Phat asses  gt   lt  Juicy Titties\n",
       "2017-10-08 21:16:08    Me gustabas pq eras distinto... ahora eres igu...\n",
       "2017-10-09 00:17:01    Supporting local music   :  Hangin  with THIS ...\n",
       "2017-10-09 01:36:38    i love when my friends travel down south to se...\n",
       "2017-10-09 02:41:03    People who only reach out to you when it s con...\n",
       "2017-10-09 04:25:37    ur always confusin sis but  lt 3 u https://t.c...\n",
       "2017-10-09 14:05:04    This is why I hop around to my family member  ...\n",
       "2017-10-09 14:30:35    Let your dreams be bigger then your fears, and...\n",
       "2017-10-09 18:11:42    gettin a good nod in at work   lt  gt  gt  gt ...\n",
       "2017-10-09 20:34:54     lt SWIPE gt  Trophy Wife      #prettyBaby #me...\n",
       "2017-10-09 21:33:01    #GSOTD  #Gym Song of the Day :  I  lt 3 EC2  -...\n",
       "2017-10-10 01:30:43    Sad for my #seminoles it s been a rough year  ...\n",
       "2017-10-10 02:19:56    H A P P Y  B I R T H D A Y   j.j_loretto #brot...\n",
       "2017-10-10 03:28:24    prayers for Christian s family and to those st...\n",
       "2017-10-10 09:57:38    @corbynbesson November 8 is my birthday so can...\n",
       "2017-10-10 10:37:00                         lt 3 https://t.co/cYlXAT1b1Z\n",
       "2017-10-10 14:19:08    A baby is a beautiful blessing  lt 3 With Jess...\n",
       "2017-10-10 16:39:08    Los dramas y griter os de toda mi familia junt...\n",
       "2017-10-10 20:45:06     @Impeach_D_Trump Stand by your fu lt king moron.\n",
       "2017-10-10 23:51:51                       @digamecampos mucho amor  lt 3\n",
       "Name: tweet_text, Length: 465, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_full[tweet_full.tweet_text.str.contains(\"\\\\blt\\\\b\")].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tweet_full.loc[\"2017-09-10 09:00:00\":\"2017-09-11 09:00:00\"].tweet_text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_count = word_list[word_list.index.str[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_words = hashtags_count.index.str[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the                        5790\n",
       "@                          4370\n",
       "i                          4057\n",
       "to                         3717\n",
       "a                          3586\n",
       "of                         3394\n",
       "in                         3178\n",
       "and                        3034\n",
       "                           2811\n",
       "is                         2761\n",
       "my                         2334\n",
       "florida                    2124\n",
       "this                       2011\n",
       "s                          1948\n",
       "#hurricaneirma             1913\n",
       "for                        1859\n",
       "from                       1806\n",
       "you                        1741\n",
       "it                         1612\n",
       "we                         1607\n",
       "on                         1543\n",
       "#irma                      1527\n",
       "at                         1505\n",
       "reports                    1386\n",
       "t                          1351\n",
       "irma                       1289\n",
       "hurricane                  1257\n",
       "are                        1166\n",
       "mph                        1160\n",
       "asos                       1118\n",
       "                           ... \n",
       "season...                     1\n",
       "ndolos                        1\n",
       "@wnba                         1\n",
       "grrr                          1\n",
       "14:57                         1\n",
       "hawk                          1\n",
       "@miamidolphins                1\n",
       "finn                          1\n",
       "#palmaire                     1\n",
       "https://t.co/2xoetp7r1r       1\n",
       "https://t.co/bemffcoscq       1\n",
       "https://t.co/kfhv9tjues       1\n",
       "https://t.co/lytyczbpuh       1\n",
       "https://t.co/ldtx3mnyd6       1\n",
       "https://t.co/kpv3eoycbl       1\n",
       "https://t.co/dfbi3dlxdj       1\n",
       "#teelingwhiskey               1\n",
       "https://t.co/o0evrh0jzw       1\n",
       "https://t.co/6dzpwf4uj0       1\n",
       "https://t.co/3nrxtbagpn       1\n",
       "tanto,                        1\n",
       "https://t.co/dwxaxcyxqw       1\n",
       "stang                         1\n",
       "tomada                        1\n",
       "#interconmiami                1\n",
       "loooud                        1\n",
       "4yr                           1\n",
       "town....                      1\n",
       "https://t.co/aggs23oduy       1\n",
       "umbrella.                     1\n",
       "Length: 45446, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1913"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list['#hurricaneirma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['.@mayorgimenez', '.@cbs12', '.@flashgjr', '.@10newswtsp',\n",
       "       '.@richarddymond', '.@drtiajolie', '#@abc', '.@realdonaldtrump',\n",
       "       '.@andrewwulfeck:', '.@occc', '.@rborn83,', '.@miamidadecounty',\n",
       "       '.@manateesheriff', '.@deadpool1973', '-@notcampbellmatt',\n",
       "       '.@dukeenergy', '.@miamidadefire', '-@grant_gilmore', '.@jimsmallman',\n",
       "       '.@thecwsupergirl', 'w@30.', '.@goabode', 'l@s', '.@jason_lanning',\n",
       "       '.@tampaelectric', '.@nicoleebryan', '.@potus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[word_list.index.str[1]=='@'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   2811.0\n",
       "my                                 2334.0\n",
       "florida                            2124.0\n",
       "this                               2011.0\n",
       "you                                1741.0\n",
       "it                                 1612.0\n",
       "we                                 1607.0\n",
       "irma                               1289.0\n",
       "hurricane                          1257.0\n",
       "gust                               1074.0\n",
       "me                                  952.0\n",
       "power                               878.0\n",
       "fl                                  847.0\n",
       "wind                                656.0\n",
       "storm                               630.0\n",
       "our                                 563.0\n",
       "now                                 524.0\n",
       "rain                                499.0\n",
       "down                                477.0\n",
       "safe                                471.0\n",
       "miami                               468.0\n",
       "go                                  410.0\n",
       "will                                393.0\n",
       "beach                               384.0\n",
       "stay                                360.0\n",
       "tornado                             357.0\n",
       "us                                  345.0\n",
       "got                                 343.0\n",
       "winds                               317.0\n",
       "f                                   313.0\n",
       "                                    ...  \n",
       "fortmyers.                            NaN\n",
       "9september                            NaN\n",
       "doglife                               NaN\n",
       "piperlove                             NaN\n",
       "russiansinflorida                     NaN\n",
       "staybless                             NaN\n",
       "pawpaw                                NaN\n",
       "illinois                              NaN\n",
       "fortpierce,                           NaN\n",
       "uracanirma                            NaN\n",
       "btwwearesafe                          NaN\n",
       "iwon                                  NaN\n",
       "laryngitis#hurricaneirma#losttv       NaN\n",
       "mydailyinspiration                    NaN\n",
       "hotterthanthedevilsdick               NaN\n",
       "vibrantatanyage                       NaN\n",
       "ihateyouirma                          NaN\n",
       "mantra                                NaN\n",
       "itshappening                          NaN\n",
       "cholanews                             NaN\n",
       "gobucs                                NaN\n",
       "emcee                                 NaN\n",
       "expeditioneverest                     NaN\n",
       "loveislove                            NaN\n",
       "bibliotherapy                         NaN\n",
       "irma.ahora                            NaN\n",
       "lonelyplanet                          NaN\n",
       "palmaire                              NaN\n",
       "teelingwhiskey                        NaN\n",
       "interconmiami                         NaN\n",
       "Length: 4690, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[hashtag_words].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = list(combinations(list(vector_model.wv.vocab.keys()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in word_pairs:\n",
    "    edge_weight = vector_model.wv.similarity(pair[0],pair[1])\n",
    "    if edge_weight > .80:\n",
    "        tweet_graph.add_edge(pair[0],pair[1],weight=edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_graph.add_nodes_from(vector_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(tweet_graph,path=r'./tweet_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
