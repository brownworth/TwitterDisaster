{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring changes in related words over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will show how words related to a particular word will change over time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from math import ceil\n",
    "import string\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tweet_id','timestamp','tweet_text','user_id',\n",
    "           'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location',\n",
    "           'enc_url','tweet_lang','hashtags']\n",
    "tweet_full = pd.read_csv(r'./tweetCoords.csv',\n",
    "                         header=None,\n",
    "                         names=columns,\n",
    "                         parse_dates=[1],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stops = stopwords.words('english')\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True,preserve_case=False,reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing a custom text cleaner. Currently configured to remove all punctuation, _except #_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "#     convert case:\n",
    "    tweet = tweet.lower()\n",
    "#     remove URLs:\n",
    "    tweet = re.sub('https?://\\S+','',tweet)\n",
    "#     remove @mentions, including those with a leading '-' or '.' : \n",
    "    tweet = re.sub('[-\\.]?@\\w+','',tweet)\n",
    "#     remove punctuation, but not hashtags:\n",
    "    tweet = tweet.translate(tweet.maketrans('','',string.punctuation.replace(\"#\",\"\")))\n",
    "#     remove non-hashtag '#'.\n",
    "    tweet = re.sub('#\\B','',tweet)\n",
    "#     remove punctuation, including hashtags:\n",
    "#     tweet = tweet.translate(tweet.maketrans('','',string.punctuation))\n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is  a tweet with    #stuff #in it \n"
     ]
    }
   ],
   "source": [
    "re_text = \"this is ! A TWEET with @some .@random @@extra #stuff ##in IT!?@>#! \"\n",
    "print(clean_tweet(re_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the word we're comparing similarity to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"irma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting here, begin the iteration over times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = pd.DataFrame()\n",
    "tweet_date = pd.to_datetime(\"2017-09-11 00:00:00\")\n",
    "date_delta = pd.Timedelta(\"24HR\")\n",
    "end_date = pd.to_datetime(\"2017-09-12 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num_words = 20 # number of words to include in cosine similarity ordered list\n",
    "pct_occ_thresh = .001 # words must occur a number of times >= this percent of number of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently there is an incompatibility between gensim and numpy > 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-11 00:00:00: 16322 tweets (17 occurrence threshold)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-12 00:00:00: 15109 tweets (16 occurrence threshold)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/TwitterDisaster-4Cppn-LV/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for tweet_day in pd.date_range(start = tweet_date, end = end_date, freq = date_delta):\n",
    "    \n",
    "    tweet_text = tweet_full.loc[tweet_day:tweet_day + date_delta,\"tweet_text\"]\n",
    "    min_count = ceil(len(tweet_text) * pct_occ_thresh)\n",
    "    print(str(tweet_day)+\": \"+str(len(tweet_text))+\" tweets (\"+str(min_count)+\" occurrence threshold)\") # this line is just here for diagnostic purposes.\n",
    "    \n",
    "    tweets_tokens = tweet_text.apply(lambda x: [clean_tweet(word) for word in tweet_tokenizer.tokenize(x) if word not in tweet_stops])\n",
    "    \n",
    "    vector_model = Word2Vec(tweets_tokens, min_count=min_count, sg=1, window=4)\n",
    "    word_matrix = vector_model.wv[vector_model.wv.vocab]\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_matrix)\n",
    "    terms_from_range = pd.DataFrame.from_records(vector_model.wv.most_similar(search_term,topn=top_num_words),columns=[tweet_day,\"Score\"])\n",
    "    related_words = pd.concat([related_words,terms_from_range],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.get_vector(\"storm\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.similarity(\"storm\",\"rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-11 00:00:00</th>\n",
       "      <th>Score</th>\n",
       "      <th>2017-09-12 00:00:00</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>0.904861</td>\n",
       "      <td>post</td>\n",
       "      <td>0.955433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#hurricaneimra</td>\n",
       "      <td>0.882627</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0.943001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bye</td>\n",
       "      <td>0.857680</td>\n",
       "      <td>aftermath</td>\n",
       "      <td>0.930415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftermath</td>\n",
       "      <td>0.853729</td>\n",
       "      <td>#irma</td>\n",
       "      <td>0.930365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel</td>\n",
       "      <td>0.849519</td>\n",
       "      <td>#hurricaneirma</td>\n",
       "      <td>0.911921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.849077</td>\n",
       "      <td>#hurricane</td>\n",
       "      <td>0.895040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#afterirma</td>\n",
       "      <td>0.846328</td>\n",
       "      <td>survived</td>\n",
       "      <td>0.894168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#naples</td>\n",
       "      <td>0.830033</td>\n",
       "      <td>#hurricaneirma2017</td>\n",
       "      <td>0.881420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>affected</td>\n",
       "      <td>0.829730</td>\n",
       "      <td>#florida</td>\n",
       "      <td>0.859789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#irmageddon</td>\n",
       "      <td>0.829599</td>\n",
       "      <td>#irmahurricane</td>\n",
       "      <td>0.856164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#irmahurricane2017</td>\n",
       "      <td>0.826382</td>\n",
       "      <td>#nopower</td>\n",
       "      <td>0.856008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#irmahurricane</td>\n",
       "      <td>0.825774</td>\n",
       "      <td>storm</td>\n",
       "      <td>0.844470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#hurricanirma</td>\n",
       "      <td>0.825189</td>\n",
       "      <td>#aftermath</td>\n",
       "      <td>0.844088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>live</td>\n",
       "      <td>0.823180</td>\n",
       "      <td>well</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>survived</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>made</td>\n",
       "      <td>0.834149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>party</td>\n",
       "      <td>0.820101</td>\n",
       "      <td>#afterirma</td>\n",
       "      <td>0.833231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#prayforflorida</td>\n",
       "      <td>0.817729</td>\n",
       "      <td>internet</td>\n",
       "      <td>0.826490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>passing</td>\n",
       "      <td>0.817363</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.821876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>0.817248</td>\n",
       "      <td>#beach</td>\n",
       "      <td>0.819348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#storm</td>\n",
       "      <td>0.816579</td>\n",
       "      <td>update</td>\n",
       "      <td>0.818612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-11 00:00:00     Score 2017-09-12 00:00:00     Score\n",
       "0                 post  0.904861                post  0.955433\n",
       "1       #hurricaneimra  0.882627           hurricane  0.943001\n",
       "2                  bye  0.857680           aftermath  0.930415\n",
       "3            aftermath  0.853729               #irma  0.930365\n",
       "4                hotel  0.849519      #hurricaneirma  0.911921\n",
       "5                  cat  0.849077          #hurricane  0.895040\n",
       "6           #afterirma  0.846328            survived  0.894168\n",
       "7              #naples  0.830033  #hurricaneirma2017  0.881420\n",
       "8             affected  0.829730            #florida  0.859789\n",
       "9          #irmageddon  0.829599      #irmahurricane  0.856164\n",
       "10  #irmahurricane2017  0.826382            #nopower  0.856008\n",
       "11      #irmahurricane  0.825774               storm  0.844470\n",
       "12       #hurricanirma  0.825189          #aftermath  0.844088\n",
       "13                live  0.823180                well  0.842000\n",
       "14            survived  0.820700                made  0.834149\n",
       "15               party  0.820101          #afterirma  0.833231\n",
       "16     #prayforflorida  0.817729            internet  0.826490\n",
       "17             passing  0.817363               clean  0.821876\n",
       "18                   9  0.817248              #beach  0.819348\n",
       "19              #storm  0.816579              update  0.818612"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-09-11 00:00:00</th>\n",
       "      <th>2017-09-12 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#hurricaneimra</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bye</td>\n",
       "      <td>aftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftermath</td>\n",
       "      <td>#irma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel</td>\n",
       "      <td>#hurricaneirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat</td>\n",
       "      <td>#hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#afterirma</td>\n",
       "      <td>survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#naples</td>\n",
       "      <td>#hurricaneirma2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>affected</td>\n",
       "      <td>#florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#irmageddon</td>\n",
       "      <td>#irmahurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#irmahurricane2017</td>\n",
       "      <td>#nopower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#irmahurricane</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#hurricanirma</td>\n",
       "      <td>#aftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>live</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>survived</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>party</td>\n",
       "      <td>#afterirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#prayforflorida</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>passing</td>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>#beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#storm</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2017-09-11 00:00:00 2017-09-12 00:00:00\n",
       "0                 post                post\n",
       "1       #hurricaneimra           hurricane\n",
       "2                  bye           aftermath\n",
       "3            aftermath               #irma\n",
       "4                hotel      #hurricaneirma\n",
       "5                  cat          #hurricane\n",
       "6           #afterirma            survived\n",
       "7              #naples  #hurricaneirma2017\n",
       "8             affected            #florida\n",
       "9          #irmageddon      #irmahurricane\n",
       "10  #irmahurricane2017            #nopower\n",
       "11      #irmahurricane               storm\n",
       "12       #hurricanirma          #aftermath\n",
       "13                live                well\n",
       "14            survived                made\n",
       "15               party          #afterirma\n",
       "16     #prayforflorida            internet\n",
       "17             passing               clean\n",
       "18                   9              #beach\n",
       "19              #storm              update"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_words.iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bdamage\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text[(tweet_text.str.contains(r\"\\bstorm\\b\",regex=True)) & (tweet_text.str.contains(r\"\\bhelping\\b\",regex=True))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing words to hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tweet_full.loc[\"2017-09-11 00:00:00\":\"2017-09-12 00:00:00\"].tweet_text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_count = word_list[word_list.index.str[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_words = hashtags_count.index.str[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list['#hurricane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['.@realdonaldtrump', 'l@s', '.@tecoenergy', '.@seminoleso',\n",
       "       '.@attcares', '.@10newswtsp', '.@marlinspark', '.@cargo5',\n",
       "       '.@andrewwulfeck:', '.@nevilleray', '-@notcampbellmatt',\n",
       "       '.@miamidadefire', '-@theslumpgod', '.@rborn83,', '.@insidefpl',\n",
       "       '.@imkristenbell', '.@wflanightphotog', '.@ejmccrane:', '.@jorgeebro',\n",
       "       '.@ejmccrane', '.@drtiajolie', '.@tampaelectric'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[word_list.index.str[1]=='@'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the                       4587.0\n",
       "                          2194.0\n",
       "we                        1515.0\n",
       "florida                   1295.0\n",
       "#hurricaneirma            1113.0\n",
       "power                      913.0\n",
       "#irma                      910.0\n",
       "irma                       857.0\n",
       "hurricane                  749.0\n",
       "me                         688.0\n",
       "our                        514.0\n",
       "wind                       460.0\n",
       "fl                         450.0\n",
       "what                       422.0\n",
       "after                      403.0\n",
       "f                          335.0\n",
       "storm                      332.0\n",
       "got                        319.0\n",
       "miami                      291.0\n",
       "good                       279.0\n",
       "house                      275.0\n",
       "one                        271.0\n",
       "safe                       270.0\n",
       "rain                       265.0\n",
       "us                         261.0\n",
       "go                         260.0\n",
       "open                       258.0\n",
       "beach                      248.0\n",
       "people                     243.0\n",
       "through                    235.0\n",
       "                           ...  \n",
       "newbutready                  NaN\n",
       "morningafter                 NaN\n",
       "bestgirlfriend               NaN\n",
       "fortlauderdaletv             NaN\n",
       "mustanggt                    NaN\n",
       "theperfectrealtor4you        NaN\n",
       "familyartphotography         NaN\n",
       "floridaproblems              NaN\n",
       "needit                       NaN\n",
       "whyiride                     NaN\n",
       "severestorm9                 NaN\n",
       "firststrike                  NaN\n",
       "bemindful                    NaN\n",
       "wheresthebeer                NaN\n",
       "windgusts                    NaN\n",
       "scrabble#hurricaneirma       NaN\n",
       "locos                        NaN\n",
       "thedeuce                     NaN\n",
       "funinkeywest                 NaN\n",
       "itsnotoveryet                NaN\n",
       "mxgp                         NaN\n",
       "mundorebelde                 NaN\n",
       "posthurricaneirma            NaN\n",
       "keywestpride                 NaN\n",
       "maturity                     NaN\n",
       "nodeliveries                 NaN\n",
       "northmiamibeach              NaN\n",
       "itstrue                      NaN\n",
       "deadbugging                  NaN\n",
       "camilito                     NaN\n",
       "Length: 3530, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[hashtag_words].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = list(combinations(list(vector_model.wv.vocab.keys()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in word_pairs:\n",
    "    edge_weight = vector_model.wv.similarity(pair[0],pair[1])\n",
    "    if edge_weight > .85:\n",
    "        tweet_graph.add_edge(pair[0],pair[1],weight=edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_graph.add_nodes_from(vector_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(tweet_graph,path=r'./tweet_graph.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
