{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = pd.read_csv(r'./tweetsText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'text'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Ocala: 7:50pm: sunset\n",
       "1    Wind 2.0 mph ESE. Barometer 30.013 in, Steady....\n",
       "2                  Where words fall....music speaks   \n",
       "3    First @TBBuccaneers with my bride @carrie_duna...\n",
       "4    Wow. That was rough. It s basically drinking a...\n",
       "5    I can t even watch #Diana20 programmes because...\n",
       "6                          Gainesville: 7:51pm: sunset\n",
       "7    Exactly 4hrs til  my blessings... @ The World ...\n",
       "8    I m at Louis Pappas Market Cafe: Shoppes at Ci...\n",
       "9    Don t try  amp  talk 2 me when it s convenient...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head(10).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TfIdf - Term frequency/Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english')) | set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vector = TfidfVectorizer(analyzer='word',stop_words=stopWords).fit_transform(tweet_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<784322x862839 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6706018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.2693774, 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vector[0:10000].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tweet_vector[0:5], tweet_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob will determine the language of text, but requires that the analyzed text be at least 3 characters. For example, tweet below is causing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_text.iloc[756,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head(31).apply(lambda x: TextBlob(x['text']).detect_language(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.iloc[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a function to preserve the short tweets, and avoid the error due to string length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLang(text_sample):\n",
    "    if len(text_sample) < 3:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return TextBlob(text_sample).detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a timeout issue when processing large amounts of tweets. May be caused by API limits? Testing with increasing numbers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lang = tweet_text[:10000].apply(lambda x: getLang(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language processing seems to be inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lang.groupby(tweet_lang).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = tweet_text.text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[tweet_words.index.str.len() > 3][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[~(tweet_words.index.isin(stop_list))].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[~(tweet_words.index.isin(stop_list)) & (tweet_words.index.str.len() > 3)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Sentiment Analysis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head(10).apply(lambda x: TextBlob(x['text']).sentiment.polarity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head(10).apply(lambda x: TextBlob(x['text']).sentiment.subjectivity,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with a large sentiment analysis dataset. Attempting to use the Twitter Sentiment Analysis Dataset Corpus obtained from http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Corpus has some extraneous quotation marks that affect parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# new_file = []\n",
    "\n",
    "# re_string = '^(\\d+,\\d+,\\w+,)(.+)$'\n",
    "# g = open('.\\sentiment_corrected.csv','w')\n",
    "# g.seek(0)\n",
    "# with open('.\\Sentiment Analysis Dataset.csv','r') as f:\n",
    "#     lines = f.readlines()\n",
    "       \n",
    "# for line in lines:\n",
    "#     line = line.replace('\"',\"'\")\n",
    "#     line = re.sub(re_string,r'\\1\"\\2\"',line)\n",
    "#     g.writelines(line)\n",
    "\n",
    "# f.close()\n",
    "# g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_corpus = pd.read_csv(r'./sentiment_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus text is in alphabetical order. Using this to experiment with 60/20/20 split for train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_sample = np.split(twitter_corpus.sample(frac=1),[int(.6*len(twitter_corpus)),int(.8*len(twitter_corpus))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
