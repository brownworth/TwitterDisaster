{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = pd.read_csv(r'./tweetsText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Ocala: 7:50pm: sunset\n",
       "1    Wind 2.0 mph ESE. Barometer 30.013 in, Steady....\n",
       "2                  Where words fall....music speaks   \n",
       "3    First @TBBuccaneers with my bride @carrie_duna...\n",
       "4    Wow. That was rough. It s basically drinking a...\n",
       "5    I can t even watch #Diana20 programmes because...\n",
       "6                          Gainesville: 7:51pm: sunset\n",
       "7    Exactly 4hrs til  my blessings... @ The World ...\n",
       "8    I m at Louis Pappas Market Cafe: Shoppes at Ci...\n",
       "9    Don t try  amp  talk 2 me when it s convenient...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head(10).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784322 entries, 0 to 784321\n",
      "Data columns (total 2 columns):\n",
      "tweet_id    784322 non-null int64\n",
      "text        784322 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet_text.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TfIdf - Term frequency/Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english')) | set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vector = TfidfVectorizer(analyzer='word',stop_words=stopWords).fit_transform(tweet_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<784322x862839 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6706018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.2693774, 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vector[0:10000].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tweet_vector[0:5], tweet_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1960804, 1899055, 1899056, ..., 3137292, 1568646, 2352969])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob will determine the language of text, but requires that the analyzed text be at least 3 characters. For example, tweet below is causing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_text.iloc[756,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     en\n",
       "1     en\n",
       "2     en\n",
       "3     en\n",
       "4     en\n",
       "5     en\n",
       "6     en\n",
       "7     en\n",
       "8     en\n",
       "9     en\n",
       "10    en\n",
       "11    en\n",
       "12    en\n",
       "13    en\n",
       "14    en\n",
       "15    en\n",
       "16    en\n",
       "17    en\n",
       "18    en\n",
       "19    en\n",
       "20    en\n",
       "21    en\n",
       "22    en\n",
       "23    en\n",
       "24    en\n",
       "25    en\n",
       "26    en\n",
       "27    en\n",
       "28    en\n",
       "29    en\n",
       "30    pt\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head(31).apply(lambda x: TextBlob(x['text']).detect_language(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                                   903407384160346113\n",
       "text        @jaguairs Passei no lugar que Loren costuma qu...\n",
       "Name: 30, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.iloc[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a function to preserve the short tweets, and avoid the error due to string length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLang(text_sample):\n",
    "    if len(text_sample) < 3:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return TextBlob(text_sample).detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a timeout issue when processing large amounts of tweets. May be caused by API limits? Testing with increasing numbers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text['Lang'] = tweet_text[:10].apply(lambda x: getLang(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to circumvent the API limitations with an iterator. (Using the `stop_point` variable to check on progress, and start over later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.to_csv(r'./tweets_with_lang.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_point = 36605\n",
    "for i in range(stop_point,tweet_text.shape[0]):\n",
    "    tweet_text.iloc[i,2] = getLang(tweet_text.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                     904056350472273921\n",
       "text        @lolitathelionn come over today :- \n",
       "Lang                                         en\n",
       "Name: 36604, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text[tweet_text['Lang'].notnull()].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                                   904056357153902593\n",
       "text        Taco City      awesome. Enjoyed celebrating th...\n",
       "Lang                                                      NaN\n",
       "Name: 36605, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.iloc[36605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lang\n",
       "af          12\n",
       "ar          57\n",
       "az           3\n",
       "bg           8\n",
       "bs           6\n",
       "ca          25\n",
       "ceb          5\n",
       "co           2\n",
       "cs           3\n",
       "cy          21\n",
       "da          39\n",
       "de          54\n",
       "el          14\n",
       "en       33170\n",
       "eo           8\n",
       "es        1912\n",
       "et          14\n",
       "eu           5\n",
       "fi          27\n",
       "fr          76\n",
       "fy          15\n",
       "ga           5\n",
       "gd           6\n",
       "gl          34\n",
       "ha           1\n",
       "haw         12\n",
       "hi          23\n",
       "hmn          5\n",
       "hr           6\n",
       "ht          33\n",
       "         ...  \n",
       "lv          12\n",
       "mg          10\n",
       "mi          71\n",
       "ms          12\n",
       "mt           9\n",
       "nl          41\n",
       "no          24\n",
       "ny           1\n",
       "pl          20\n",
       "pt         288\n",
       "ro          36\n",
       "ru           9\n",
       "sk          10\n",
       "sl           2\n",
       "sm           1\n",
       "sn           1\n",
       "so          22\n",
       "sq          16\n",
       "st           2\n",
       "su           8\n",
       "sv          55\n",
       "sw           8\n",
       "tl          66\n",
       "tr          12\n",
       "uz          10\n",
       "vi          20\n",
       "xh           2\n",
       "yo           2\n",
       "zh-CN        4\n",
       "zu          13\n",
       "Name: Lang, Length: 72, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text[tweet_text['Lang'].notnull()]['Lang'].groupby(tweet_text['Lang']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language processing seems to be inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lang.groupby(tweet_lang).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = tweet_text.text.str.lower().str.split(r'\\s+',expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[tweet_words.index.str.len() > 3][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[~(tweet_words.index.isin(stop_list))].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words[~(tweet_words.index.isin(stop_list)) & (tweet_words.index.str.len() > 3)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Sentiment Analysis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head(10).apply(lambda x: TextBlob(x['text']).sentiment.polarity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.head(10).apply(lambda x: TextBlob(x['text']).sentiment.subjectivity,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with a large sentiment analysis dataset. Attempting to use the Twitter Sentiment Analysis Dataset Corpus obtained from http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Corpus has some extraneous quotation marks that affect parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# new_file = []\n",
    "\n",
    "# re_string = '^(\\d+,\\d+,\\w+,)(.+)$'\n",
    "# g = open('.\\sentiment_corrected.csv','w')\n",
    "# g.seek(0)\n",
    "# with open('.\\Sentiment Analysis Dataset.csv','r') as f:\n",
    "#     lines = f.readlines()\n",
    "       \n",
    "# for line in lines:\n",
    "#     line = line.replace('\"',\"'\")\n",
    "#     line = re.sub(re_string,r'\\1\"\\2\"',line)\n",
    "#     g.writelines(line)\n",
    "\n",
    "# f.close()\n",
    "# g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_corpus = pd.read_csv(r'./sentiment_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus text is in alphabetical order. Using this to experiment with 60/20/20 split for train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_sample = np.split(twitter_corpus.sample(frac=1),[int(.6*len(twitter_corpus)),int(.8*len(twitter_corpus))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
